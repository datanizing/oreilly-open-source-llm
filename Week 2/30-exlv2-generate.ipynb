{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3a5070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ceb4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exllamav2 import(\n",
    "    ExLlamaV2,\n",
    "    ExLlamaV2Config,\n",
    "    ExLlamaV2Cache,\n",
    "    ExLlamaV2Cache_8bit,\n",
    "    ExLlamaV2Tokenizer,\n",
    "    model_init,\n",
    ")\n",
    "\n",
    "from exllamav2.generator import (\n",
    "    ExLlamaV2StreamingGenerator,\n",
    "    ExLlamaV2BaseGenerator,\n",
    "    ExLlamaV2Sampler\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d1611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = \"/home/cwinkler/oreilly/models/Mistral-7B-instruct-exl2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c832042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ExLlamaV2Config()\n",
    "config.model_dir = model_directory\n",
    "config.prepare()\n",
    "\n",
    "model = ExLlamaV2(config)\n",
    "print(\"Loading model: \" + model_directory)\n",
    "\n",
    "cache = ExLlamaV2Cache(model, lazy = True)\n",
    "model.load_autosplit(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c74685",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ExLlamaV2Tokenizer(config)\n",
    "\n",
    "# Initialize generator\n",
    "\n",
    "generator = ExLlamaV2StreamingGenerator(model, cache, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafa88ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = ExLlamaV2Sampler.Settings()\n",
    "settings.temperature = 0.1\n",
    "settings.top_k = 50\n",
    "settings.top_p = 0.8\n",
    "settings.token_repetition_penalty = 1.05\n",
    "settings.disallow_tokens(tokenizer, [tokenizer.eos_token_id])\n",
    "\n",
    "max_new_tokens = 600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de26fa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Tell me about O'Reilly Safari!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7b7ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(prompt)\n",
    "prompt_tokens = input_ids.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33684d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.warmup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538e5715",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.set_stop_conditions([])\n",
    "generator.begin_stream(input_ids, settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2414b06b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "generated_tokens = 0\n",
    "while True:\n",
    "    chunk, eos, _ = generator.stream()\n",
    "    generated_tokens += 1\n",
    "    print (chunk, end = \"\")\n",
    "    sys.stdout.flush()\n",
    "    if eos or generated_tokens == max_new_tokens: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca355721",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2669131c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
