{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7744f59-2e1b-4496-a38c-b5f930d6a6e9",
   "metadata": {},
   "source": [
    "# Generate text using the model Qwen-8B\n",
    "\n",
    "In this notebook, we use the generative large language model\n",
    "[Qwen-8B](https://huggingface.co/Qwen/Qwen3-8B). This is a hybrid model\n",
    "which can use \"thinking\" mode, but it is also possible to omit it.\n",
    "\n",
    "Qwen-8B has 8 billion parameters, so with 2 bytes per parameter\n",
    "(`bfloat16`) we expect a memory usage of 16 GB.\n",
    "\n",
    "We will use the chat template for Qwen-8B and different prompts. You\n",
    "can observe the behavior from LLMs that they produce different answers\n",
    "for the same prompts. You will also see how can you can avoid that\n",
    "and get reproducible answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92465c5-4c6a-4e7d-abe3-f37aebdf5d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.get_device_properties(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4a2b95-81fc-4339-8934-8bfda99c2f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "# model and tokenizer must match\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             dtype=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703a2bb5-dcb8-4559-bb36-5e3cc8f48a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer the model to the GPU\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2d5195-7855-47dd-84b5-076b730fe1c2",
   "metadata": {},
   "source": [
    "Check the memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e627be79-3160-4e44-9cff-ec904582e300",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f42f88d-c92c-445d-9b16-1e0be36f6c49",
   "metadata": {},
   "source": [
    "Prompting the LLM works via the a special array which contains `dict`s. Each `dict`\n",
    "has two keys, one is for the `role`, the other for the `content`. This structure \n",
    "is the same for all LLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ac7d1d-490f-4d61-acb0-5068e2d694e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Tell me about O'Reilly online learning\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6507a327-4fe7-437b-910a-289137355b8b",
   "metadata": {},
   "source": [
    "The template itself however differs considerably. Fortunately,\n",
    "it is included in the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8591eb4e-1760-4fd1-ad7f-4257ce69df72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631e357-8071-4889-aab5-8562cf7d0304",
   "metadata": {},
   "source": [
    "The tokenizer also know how to apply the template and the result is much easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6d569d-4395-4d71-99ad-3386357ccc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    enable_thinking=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cc949d-5f93-4883-9a87-d92f35612ad0",
   "metadata": {},
   "source": [
    "This can now be used for instructing the LLM to `generate`\n",
    "which invokes the text completion mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57006eee-9540-46e9-abf7-4fb97bb3f5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "tokens = tokenizer(text, return_tensors='pt')\n",
    "output = model.generate(inputs=tokens.input_ids.cuda(), attention_mask=tokens.attention_mask.cuda(),\n",
    "                        temperature=0.7, \n",
    "                        do_sample=True, top_p=0.95, top_k=40, \n",
    "                        max_new_tokens=512)\n",
    "used = time.time() - start\n",
    "tps = len(output[0]) / used\n",
    "print(tokenizer.decode(output[0]))\n",
    "print(f\"{used} seconds, {tps} tokens/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c949fc-d504-48f8-8efc-756cc46e7c9c",
   "metadata": {},
   "source": [
    "Due to the positive temperature, we get different responses (sampling):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cb3450",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "output = model.generate(inputs=tokens.input_ids.cuda(), attention_mask=tokens.attention_mask.cuda(),\n",
    "                        temperature=0.7, \n",
    "                        do_sample=True, top_p=0.95, top_k=40, \n",
    "                        max_new_tokens=512)\n",
    "used = time.time() - start\n",
    "tps = len(output[0]) / used\n",
    "print(tokenizer.decode(output[0]))\n",
    "print(f\"{used} seconds, {tps} tokens/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e79c26b-3b97-4638-a2a9-c8314c74e7b0",
   "metadata": {},
   "source": [
    "If we set `do_sample=False`, we get reproducible results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008d4b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "output = model.generate(inputs=tokens.input_ids.cuda(), attention_mask=tokens.attention_mask.cuda(),\n",
    "                        do_sample=False, max_new_tokens=512)\n",
    "used = time.time() - start\n",
    "tps = len(output[0]) / used\n",
    "print(tokenizer.decode(output[0]))\n",
    "print(f\"{used} seconds, {tps} tokens/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd4f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "output = model.generate(inputs=tokens.input_ids.cuda(), attention_mask=tokens.attention_mask.cuda(),\n",
    "                        do_sample=False, max_new_tokens=512)\n",
    "used = time.time() - start\n",
    "tps = len(output[0]) / used\n",
    "print(tokenizer.decode(output[0]))\n",
    "print(f\"{used} seconds, {tps} tokens/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f170b8-6f6c-4ab1-9019-258813bcc4f3",
   "metadata": {},
   "source": [
    "Take a look at what happens when we ask for knowledge which is not in the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6877f2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explain the GRPO training method for LLMs!\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Only answer if you are absolutely sure. Otherwise tell me that you don't know the answer\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af8ea1-63ee-4d9d-b997-fec532175b57",
   "metadata": {},
   "source": [
    "Slight change: take a look at the system prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464aa607-ac48-461d-9818-77a021e112af",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Writen an abstract about 'Frontiers in GRPO training LLMs'!\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a creative researcher.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ed5fae-798f-40e0-b2da-877693b66c90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollm",
   "language": "python",
   "name": "ollm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
