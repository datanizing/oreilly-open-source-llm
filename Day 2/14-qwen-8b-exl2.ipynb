{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fced9f05-c6e4-4714-a901-2298dc6eee9e",
   "metadata": {},
   "source": [
    "# Using Qwen-8B with exl2 quantization and kernel\n",
    "\n",
    "ExllamaV2 is a powerful quantization technique which works\n",
    "with dedicated kernels. Unfortunately, it is not integrated\n",
    "into the Hugging Face ecosystem.\n",
    "\n",
    "However, it is really fast! Therefore, we will take a look\n",
    "at this notebook and see what's different. The real *revolution*\n",
    "works behind the scenes. By using a shortcut (skipping\n",
    "`transformers` and `torch` and going directly via \n",
    "`triton` to CUDA), specialized kernels for each language model\n",
    "are highly optimized and contribute to the excellent performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bf86b3-83fa-4971-8825-dfdb3e51e69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exllamav2 import ExLlamaV2, ExLlamaV2Config, ExLlamaV2Cache, ExLlamaV2Tokenizer, Timer\n",
    "from exllamav2.generator import ExLlamaV2DynamicGenerator, ExLlamaV2Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb0ac29-eec5-4b4b-bda4-a365af625b49",
   "metadata": {},
   "source": [
    "Exllama has a cache which speeds things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550f432b-0d7b-4373-846f-4e6b4d3f4c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cache_tokens = 16384"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871cf867-3fa2-4c0b-8987-213e35fe7f1c",
   "metadata": {},
   "source": [
    "You have to download Exllama models manually (from Hugging Face) and work with local directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da36e32-91e6-493b-b5cd-75d26ac48fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/home/cwinkler/oreilly/models/Qwen3-8B-exl2\"\n",
    "config = ExLlamaV2Config(model_dir)\n",
    "config.arch_compat_overrides()\n",
    "model = ExLlamaV2(config)\n",
    "cache = ExLlamaV2Cache(model, max_seq_len = total_cache_tokens, lazy = True)\n",
    "model.load_autosplit(cache, progress = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd466903-25e6-468a-b8d7-856fa2f8e11a",
   "metadata": {},
   "source": [
    "Of course, this models also have a tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd009b1-b248-45b4-a0f2-85a527547b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ExLlamaV2Tokenizer(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6149332-4411-4054-8a58-9a3968138ef2",
   "metadata": {},
   "source": [
    "However, it is not so easy to apply the chat template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3f7cdf-054d-4b1d-9f04-56b3846417bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\n",
    "<|im_start|>user\\nTell me about O'Reilly online learning!<|im_end|>\n",
    "<|im_start|>assistant\n",
    "<think>\n",
    "\n",
    "</think>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c23dbdd-cc22-4337-a809-44e9a7264334",
   "metadata": {},
   "source": [
    "Some parameters for generating the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a0fca5-5478-45e9-9837-f5fb9d1acec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 1024\n",
    "gen_settings = ExLlamaV2Sampler.Settings.greedy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ff2d51-ddda-4afa-9fff-79ba7f90dc2e",
   "metadata": {},
   "source": [
    "Instantiation and warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1e7c28-ac43-45e2-be98-6fb1f2c8bfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = ExLlamaV2DynamicGenerator(\n",
    "    model = model,\n",
    "    cache = cache,\n",
    "    tokenizer = tokenizer,\n",
    ")\n",
    "generator.warmup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0b04ed-1963-4569-ae05-5d9f1f042081",
   "metadata": {},
   "source": [
    "This is the actual text generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd108be6-00b3-4b38-a596-ef6dd4fd52df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer() as used:\n",
    "    output = generator.generate(\n",
    "        prompt = prompt,\n",
    "        max_new_tokens = max_new_tokens,\n",
    "        encode_special_tokens = True,\n",
    "        gen_settings = gen_settings\n",
    "    )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae699e7e-7a94-444a-8abf-19efbb0f6506",
   "metadata": {},
   "source": [
    "It is fast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5bc890-b957-4531-981b-28c374e4b476",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens / used.interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c31985-8588-4f5a-877e-282fa5e622cf",
   "metadata": {},
   "source": [
    "And uses less RAM than the full model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e527b14-2dfd-4dbb-92eb-bbae82a22374",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b67ca11-2dbe-4ae9-9f15-76a31c41815e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollm",
   "language": "python",
   "name": "ollm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
