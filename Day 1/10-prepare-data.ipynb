{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59bc0de5-d3c1-4ea6-a293-630c99fd22fd",
   "metadata": {},
   "source": [
    "# Data preparation for semantic sentence similarity \n",
    "\n",
    "When we want to retrieve text data, we need text documents. For this course, I chose the [General debate of the seventy-eighth session of the United Nations General Assembly](https://en.wikipedia.org/wiki/General_debate_of_the_seventy-eighth_session_of_the_United_Nations_General_Assembly) which is free and available online. In the general debates, each country can give a (long) speech)\n",
    "\n",
    "Unfortunately, there are a few issues with these documents:\n",
    "\n",
    "* Sometimes, `.` has been used in wronge places. I tried to correct that.\n",
    "* The debates are long, retrieving the correct debate is not sufficient.\n",
    "* Therefore, we have to work with smaller entities.\n",
    "\n",
    "In this first notebook, the debates are separated in sentences. Normally, you would then combine several sentences as fragments (chunking). But in this case, the sentences are already quite long and self-contained. Therefore, we keep the sentences as entities.\n",
    "\n",
    "Sentence segmentation sounds easy, but we cannot just use a `.` to split the sentences. This would not work for \"Mr. X\" or similar contructs. Instead, we use [spacy](https://spacy.io) as a tool for linguistic analysis which also can perform this sentence splitting.\n",
    "\n",
    "This notebook loads the data, splits the sentences and saves them in `json` format so we can use them in the later notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64ece1d-7558-4fb6-a98b-ad06283bb898",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c08c809-10c6-42aa-9a01-0477ca426b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5ecf2e-a314-450a-917b-c4b524aff977",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for n in glob.glob(\"un/TXT/Session 78 - 2023/*.txt\"):\n",
    "    data.append({\"country\": os.path.basename(n.replace(\"_78_2023.txt\", \"\")), \"text\": open(n).read() })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0112212d-2b8e-4b38-b2c2-333799200b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532c4a6c-52d3-4436-9e45-b1e4f870baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 500)\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eb4820-19d9-4dbf-9690-d62a2df94c59",
   "metadata": {},
   "source": [
    "## Sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e32ba2-bd5e-40cc-bf91-4fe46de6a0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f37d16-5d30-4329-8824-75e8a0b647bc",
   "metadata": {},
   "source": [
    "Each language has several models which can be used to analyze text. In this case, we use a small model for English, as we are only interested in sentence segmentation. If we were interested in named entity recoginition, a larger model would be more suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3232940b-b7f3-45d1-9933-e86b5a4eff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5891c67e-f9bd-496c-b4aa-a4d904cbe3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cb5d77-1257-4e77-9ff7-638614016a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202f62ea-b081-4e3d-9719-5d934b566649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs ~ 1min\n",
    "sentences = []\n",
    "for text in tqdm(df[\"text\"]):\n",
    "    doc = nlp(text)\n",
    "    for sentence in doc.sents:\n",
    "        sentences.append(str(sentence).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d67a487-962e-4e91-ad7b-2aa87309c647",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d348e426-7359-41c3-a06b-8919d0587d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9f99b4-9a57-4f80-9dc6-e9509a75fa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"sentences.json\", \"w\") as f:\n",
    "    f.write(json.dumps(sentences))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollm",
   "language": "python",
   "name": "ollm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
