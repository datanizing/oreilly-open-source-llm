[project]
name = "ollm-env"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    "accelerate>=1.10.1",
    "autoawq>=0.2.9",
    "bitsandbytes>=0.47.0",
    "device-smi>=0.4.1",
    "exllamav2>=0.3.2",
    "flash-attn>=2.8.3",
    "flashinfer-python>=0.3.1",
    "ipykernel>=6.30.1",
    "ipython>=9.5.0",
    "ipywidgets>=8.1.7",
    "jupyter>=1.1.1",
    "langchain>=0.3.27",
    "langchain-community>=0.3.29",
    "langchain-huggingface>=0.3.1",
    "logbar>=0.0.4",
    "nvidia-ml-py>=13.580.82",
    "openai>=1.107.2",
    "optimum>=1.27.0",
    "pandas>=2.3.2",
    "pip>=25.2",
    "pymilvus>=2.6.1",
    "sentence-transformers>=5.1.0",
    "spacy>=3.8.7",
    "tantivy>=0.25.0",
    "tokenicer>=0.0.5",
    "tqdm>=4.67.1",
    "transformers>=4.56.1",
    "txtai>=9.0.0",
    "usearch>=2.21.0",
    "vllm>=0.10.2",
]

[tool.uv.extra-build-dependencies]
flash-attn = [{ requirement = "torch", match-runtime = true }]

[tool.uv.extra-build-variables]
flash-attn = { FLASH_ATTENTION_SKIP_CUDA_BUILD = "TRUE" }

